# Data warehouse with Amazon Redshift

## Introduction
___

This project is about migrating Sparkify's user base and song database from Amazon S3 bucket to a cloud data warehouse, which is Amazon Redshift for analytics. Sparkify is a music streaming app and their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

## Source Dataset
___
Source data sets are: <br>
- Song dataset: <br>
    The Song dataset contains metatdata about a song and its artist. Its files are stored in JSON format and are partitioned by the first three letters of each song's track ID.
    It resides in amazon S3 bucket, with the link below:

    <pre>s3://udacity-dend/song_data

    
- Log dataset: <br>
    The Log dataset contains log files in JSON format generated by an event simulator based on the songs dataset above. These simulate app activity logs from an imaginary music streaming app.
    It resided in amazon S3 bucket, with its link and json path specified respectively below:

    <pre>s3://udacity-dend/log_data 
    <br>s3://udacity-dend/log_json_path.json 

## Project Files
___
1. <code>sql_queries.py</code>: Contains all sql scripts for dropping and creating tables as well as inserting into tables for the staging, fact and dimensions tables.

2. <code>create_tables.py</code>: Contains the python scripts for executing the sql queries in **sql_queries.py** file that drop and create tables.

3. <code>etl.py</code>: Contains the python scripts for executing insert statements in **sql_queries.py** file, which inserts data into the staging tables as well as the facts and dimensions tables.

3. <code>dwh.cfg</code>: is a configuration file that contains credentials for creating and obtaining access to the various resources that are used in the project.

4. <code>etl_explore.ipynb</code>: is a jupyter notebook that contains python scripts for creating IAM role with policy and Redshift cluster, inorder to use the generated end point and ARN to load the data warehouse.

5. <code>sparkify_er_diagram.jpg</code>: is an entity relationship diagram that shows the schema of the data warehouse.

6. <code>README.md</code>: gives a summary of the project and an explanation of the files, and how to run them.

## Configuration Process
___

### Getting connection to Redshift (etl_explore.ipynb)
    1. Create IAM role for accessing S3 bucket from Redshift.
    2. Attach policy (AmazonS3ReadOnlyAccess) to IAM role and get the ARN (Amazon Resource Name).
    3. Create Redshift cluster and obtain the Endpoint.
    4. Update the ARN and Endpoint in the dwh.cfg

### Updating the sql queries to drop, create and insert into tables (sql_queries.py)
    1. Write DROP sql statement for the staging, fact and dimensions tables.
        The staging tables are:
        - staging_events
        - staging_songs
        The fact table is songplays.
        The dimension tables are:
        - users
        - time
        - artists
        - songs
    2. Write CREATE sql statements for creating the staging, fact and dimensions tables.
    3. Write the COPY command for extracting data from S3 to the staging tables.
    4. Write INSERT sql statements for inserting data into the fact and dimensions tables.

### Schema Diagram
![Schema Diagram](/sparkify_er_diagram.jpg)

## ETL Process
___
### This project can be divided into two phases:<br>
    1. Extract data from S3 and load to the staging tables without any transformation.
    2. load data from staging into the fact and dimensions tables.

### How to run the ETL:<br>
    After all configurations and updates, as described above, have been done, follow the below steps in the required order to run the ETL:  
    Step 1. Execute the create_tables.py file which will:
        - drop the tables if they exist, and 
        - create new tables.
    Step 2. Execute the etl.py file which will:
        - copy the song data and log data from S3 to the staging tables, and 
        - load data from the staging tables to the fact and dimensions tables.

## Song play Analysis
___

    stg_events = pd.read_sql("SELECT  COUNT(*) FROM staging_events;", conn)
    stg_songs = pd.read_sql("SELECT  COUNT(*) FROM staging_songs;", conn)
    songplay_count = pd.read_sql("SELECT  COUNT(*) FROM songplays;", conn)
    artist_count = pd.read_sql("SELECT  COUNT(*) FROM artists;", conn)
    song_count = pd.read_sql("SELECT  COUNT(*) FROM songs;", conn)
    user_count = pd.read_sql("SELECT  COUNT(*) FROM users;", conn)

    print('count of staging_events is {}.'.format(stg_events.iloc[0]['count']))
    print('count of staging_songs is {}.'.format(stg_songs.iloc[0]['count']))
    print('count from songplays is {}.'.format(songplay_count.iloc[0]['count']))
    print('count of artists is {}.'.format(artist_count.iloc[0]['count']))
    print('count of songs is {}.'.format(song_count.iloc[0]['count']))
    print('count from users is {}.'.format(user_count.iloc[0]['count']))

    count of staging_events is 8056.
    count of staging_songs is 14896.
    count from songplays is 9957.
    count of artists is 10025.
    count of songs is 14896.
    count from users is 104.




